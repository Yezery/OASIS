{"ast":null,"code":"import errCode from 'err-code';\nimport { UnixFS } from 'ipfs-unixfs';\nimport persist from '../../utils/persist.js';\nimport { encode, prepare } from '@ipld/dag-pb';\nimport parallelBatch from 'it-parallel-batch';\nimport * as rawCodec from 'multiformats/codecs/raw';\nimport * as dagPb from '@ipld/dag-pb';\nimport dagFlat from './flat.js';\nimport dagBalanced from './balanced.js';\nimport dagTrickle from './trickle.js';\nimport bufferImporterFn from './buffer-importer.js';\n\n/**\n * @typedef {import('interface-blockstore').Blockstore} Blockstore\n * @typedef {import('../../types').File} File\n * @typedef {import('../../types').ImporterOptions} ImporterOptions\n * @typedef {import('../../types').Reducer} Reducer\n * @typedef {import('../../types').DAGBuilder} DAGBuilder\n * @typedef {import('../../types').FileDAGBuilder} FileDAGBuilder\n */\n\n/**\n * @type {{ [key: string]: FileDAGBuilder}}\n */\nconst dagBuilders = {\n  flat: dagFlat,\n  balanced: dagBalanced,\n  trickle: dagTrickle\n};\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nasync function* buildFileBatch(file, blockstore, options) {\n  let count = -1;\n  let previous;\n  let bufferImporter;\n  if (typeof options.bufferImporter === 'function') {\n    bufferImporter = options.bufferImporter;\n  } else {\n    bufferImporter = bufferImporterFn;\n  }\n  for await (const entry of parallelBatch(bufferImporter(file, blockstore, options), options.blockWriteConcurrency)) {\n    count++;\n    if (count === 0) {\n      previous = entry;\n      continue;\n    } else if (count === 1 && previous) {\n      yield previous;\n      previous = null;\n    }\n    yield entry;\n  }\n  if (previous) {\n    previous.single = true;\n    yield previous;\n  }\n}\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nconst reduce = (file, blockstore, options) => {\n  /**\n   * @type {Reducer}\n   */\n  async function reducer(leaves) {\n    if (leaves.length === 1 && leaves[0].single && options.reduceSingleLeafToSelf) {\n      const leaf = leaves[0];\n      if (file.mtime !== undefined || file.mode !== undefined) {\n        // only one leaf node which is a buffer - we have metadata so convert it into a\n        // UnixFS entry otherwise we'll have nowhere to store the metadata\n        let buffer = await blockstore.get(leaf.cid);\n        leaf.unixfs = new UnixFS({\n          type: 'file',\n          mtime: file.mtime,\n          mode: file.mode,\n          data: buffer\n        });\n        buffer = encode(prepare({\n          Data: leaf.unixfs.marshal()\n        }));\n\n        // // TODO vmx 2021-03-26: This is what the original code does, it checks\n        // // the multihash of the original leaf node and uses then the same\n        // // hasher. i wonder if that's really needed or if we could just use\n        // // the hasher from `options.hasher` instead.\n        // const multihash = mh.decode(leaf.cid.multihash.bytes)\n        // let hasher\n        // switch multihash {\n        //   case sha256.code {\n        //     hasher = sha256\n        //     break;\n        //   }\n        //   //case identity.code {\n        //   //  hasher = identity\n        //   //  break;\n        //   //}\n        //   default: {\n        //     throw new Error(`Unsupported hasher \"${multihash}\"`)\n        //   }\n        // }\n        leaf.cid = await persist(buffer, blockstore, {\n          ...options,\n          codec: dagPb,\n          hasher: options.hasher,\n          cidVersion: options.cidVersion\n        });\n        leaf.size = buffer.length;\n      }\n      return {\n        cid: leaf.cid,\n        path: file.path,\n        unixfs: leaf.unixfs,\n        size: leaf.size\n      };\n    }\n\n    // create a parent node and add all the leaves\n    const f = new UnixFS({\n      type: 'file',\n      mtime: file.mtime,\n      mode: file.mode\n    });\n    const links = leaves.filter(leaf => {\n      if (leaf.cid.code === rawCodec.code && leaf.size) {\n        return true;\n      }\n      if (leaf.unixfs && !leaf.unixfs.data && leaf.unixfs.fileSize()) {\n        return true;\n      }\n      return Boolean(leaf.unixfs && leaf.unixfs.data && leaf.unixfs.data.length);\n    }).map(leaf => {\n      if (leaf.cid.code === rawCodec.code) {\n        // node is a leaf buffer\n        f.addBlockSize(leaf.size);\n        return {\n          Name: '',\n          Tsize: leaf.size,\n          Hash: leaf.cid\n        };\n      }\n      if (!leaf.unixfs || !leaf.unixfs.data) {\n        // node is an intermediate node\n        f.addBlockSize(leaf.unixfs && leaf.unixfs.fileSize() || 0);\n      } else {\n        // node is a unixfs 'file' leaf node\n        f.addBlockSize(leaf.unixfs.data.length);\n      }\n      return {\n        Name: '',\n        Tsize: leaf.size,\n        Hash: leaf.cid\n      };\n    });\n    const node = {\n      Data: f.marshal(),\n      Links: links\n    };\n    const buffer = encode(prepare(node));\n    const cid = await persist(buffer, blockstore, options);\n    return {\n      cid,\n      path: file.path,\n      unixfs: f,\n      size: buffer.length + node.Links.reduce((acc, curr) => acc + curr.Tsize, 0)\n    };\n  }\n  return reducer;\n};\n\n/**\n * @type {import('../../types').UnixFSV1DagBuilder<File>}\n */\nfunction fileBuilder(file, block, options) {\n  const dagBuilder = dagBuilders[options.strategy];\n  if (!dagBuilder) {\n    throw errCode(new Error(`Unknown importer build strategy name: ${options.strategy}`), 'ERR_BAD_STRATEGY');\n  }\n  return dagBuilder(buildFileBatch(file, block, options), reduce(file, block, options), options);\n}\nexport default fileBuilder;","map":{"version":3,"names":["errCode","UnixFS","persist","encode","prepare","parallelBatch","rawCodec","dagPb","dagFlat","dagBalanced","dagTrickle","bufferImporterFn","dagBuilders","flat","balanced","trickle","buildFileBatch","file","blockstore","options","count","previous","bufferImporter","entry","blockWriteConcurrency","single","reduce","reducer","leaves","length","reduceSingleLeafToSelf","leaf","mtime","undefined","mode","buffer","get","cid","unixfs","type","data","Data","marshal","codec","hasher","cidVersion","size","path","f","links","filter","code","fileSize","Boolean","map","addBlockSize","Name","Tsize","Hash","node","Links","acc","curr","fileBuilder","block","dagBuilder","strategy","Error"],"sources":["/Users/yezery/Oasis/OASIS/node_modules/.store/ipfs-unixfs-importer@12.0.1/node_modules/ipfs-unixfs-importer/src/dag-builder/file/index.js"],"sourcesContent":["import errCode from 'err-code'\nimport { UnixFS } from 'ipfs-unixfs'\nimport persist from '../../utils/persist.js'\nimport { encode, prepare } from '@ipld/dag-pb'\nimport parallelBatch from 'it-parallel-batch'\nimport * as rawCodec from 'multiformats/codecs/raw'\nimport * as dagPb from '@ipld/dag-pb'\n\nimport dagFlat from './flat.js'\nimport dagBalanced from './balanced.js'\nimport dagTrickle from './trickle.js'\nimport bufferImporterFn from './buffer-importer.js'\n\n/**\n * @typedef {import('interface-blockstore').Blockstore} Blockstore\n * @typedef {import('../../types').File} File\n * @typedef {import('../../types').ImporterOptions} ImporterOptions\n * @typedef {import('../../types').Reducer} Reducer\n * @typedef {import('../../types').DAGBuilder} DAGBuilder\n * @typedef {import('../../types').FileDAGBuilder} FileDAGBuilder\n */\n\n/**\n * @type {{ [key: string]: FileDAGBuilder}}\n */\nconst dagBuilders = {\n  flat: dagFlat,\n  balanced: dagBalanced,\n  trickle: dagTrickle\n}\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nasync function * buildFileBatch (file, blockstore, options) {\n  let count = -1\n  let previous\n  let bufferImporter\n\n  if (typeof options.bufferImporter === 'function') {\n    bufferImporter = options.bufferImporter\n  } else {\n    bufferImporter = bufferImporterFn\n  }\n\n  for await (const entry of parallelBatch(bufferImporter(file, blockstore, options), options.blockWriteConcurrency)) {\n    count++\n\n    if (count === 0) {\n      previous = entry\n      continue\n    } else if (count === 1 && previous) {\n      yield previous\n      previous = null\n    }\n\n    yield entry\n  }\n\n  if (previous) {\n    previous.single = true\n    yield previous\n  }\n}\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nconst reduce = (file, blockstore, options) => {\n  /**\n   * @type {Reducer}\n   */\n  async function reducer (leaves) {\n    if (leaves.length === 1 && leaves[0].single && options.reduceSingleLeafToSelf) {\n      const leaf = leaves[0]\n\n      if (file.mtime !== undefined || file.mode !== undefined) {\n        // only one leaf node which is a buffer - we have metadata so convert it into a\n        // UnixFS entry otherwise we'll have nowhere to store the metadata\n        let buffer = await blockstore.get(leaf.cid)\n\n        leaf.unixfs = new UnixFS({\n          type: 'file',\n          mtime: file.mtime,\n          mode: file.mode,\n          data: buffer\n        })\n\n        buffer = encode(prepare({ Data: leaf.unixfs.marshal() }))\n\n        // // TODO vmx 2021-03-26: This is what the original code does, it checks\n        // // the multihash of the original leaf node and uses then the same\n        // // hasher. i wonder if that's really needed or if we could just use\n        // // the hasher from `options.hasher` instead.\n        // const multihash = mh.decode(leaf.cid.multihash.bytes)\n        // let hasher\n        // switch multihash {\n        //   case sha256.code {\n        //     hasher = sha256\n        //     break;\n        //   }\n        //   //case identity.code {\n        //   //  hasher = identity\n        //   //  break;\n        //   //}\n        //   default: {\n        //     throw new Error(`Unsupported hasher \"${multihash}\"`)\n        //   }\n        // }\n        leaf.cid = await persist(buffer, blockstore, {\n          ...options,\n          codec: dagPb,\n          hasher: options.hasher,\n          cidVersion: options.cidVersion\n        })\n        leaf.size = buffer.length\n      }\n\n      return {\n        cid: leaf.cid,\n        path: file.path,\n        unixfs: leaf.unixfs,\n        size: leaf.size\n      }\n    }\n\n    // create a parent node and add all the leaves\n    const f = new UnixFS({\n      type: 'file',\n      mtime: file.mtime,\n      mode: file.mode\n    })\n\n    const links = leaves\n      .filter(leaf => {\n        if (leaf.cid.code === rawCodec.code && leaf.size) {\n          return true\n        }\n\n        if (leaf.unixfs && !leaf.unixfs.data && leaf.unixfs.fileSize()) {\n          return true\n        }\n\n        return Boolean(leaf.unixfs && leaf.unixfs.data && leaf.unixfs.data.length)\n      })\n      .map((leaf) => {\n        if (leaf.cid.code === rawCodec.code) {\n          // node is a leaf buffer\n          f.addBlockSize(leaf.size)\n\n          return {\n            Name: '',\n            Tsize: leaf.size,\n            Hash: leaf.cid\n          }\n        }\n\n        if (!leaf.unixfs || !leaf.unixfs.data) {\n          // node is an intermediate node\n          f.addBlockSize((leaf.unixfs && leaf.unixfs.fileSize()) || 0)\n        } else {\n          // node is a unixfs 'file' leaf node\n          f.addBlockSize(leaf.unixfs.data.length)\n        }\n\n        return {\n          Name: '',\n          Tsize: leaf.size,\n          Hash: leaf.cid\n        }\n      })\n\n    const node = {\n      Data: f.marshal(),\n      Links: links\n    }\n    const buffer = encode(prepare(node))\n    const cid = await persist(buffer, blockstore, options)\n\n    return {\n      cid,\n      path: file.path,\n      unixfs: f,\n      size: buffer.length + node.Links.reduce((acc, curr) => acc + curr.Tsize, 0)\n    }\n  }\n\n  return reducer\n}\n\n/**\n * @type {import('../../types').UnixFSV1DagBuilder<File>}\n */\nfunction fileBuilder (file, block, options) {\n  const dagBuilder = dagBuilders[options.strategy]\n\n  if (!dagBuilder) {\n    throw errCode(new Error(`Unknown importer build strategy name: ${options.strategy}`), 'ERR_BAD_STRATEGY')\n  }\n\n  return dagBuilder(buildFileBatch(file, block, options), reduce(file, block, options), options)\n}\n\nexport default fileBuilder\n"],"mappings":"AAAA,OAAOA,OAAO,MAAM,UAAU;AAC9B,SAASC,MAAM,QAAQ,aAAa;AACpC,OAAOC,OAAO,MAAM,wBAAwB;AAC5C,SAASC,MAAM,EAAEC,OAAO,QAAQ,cAAc;AAC9C,OAAOC,aAAa,MAAM,mBAAmB;AAC7C,OAAO,KAAKC,QAAQ,MAAM,yBAAyB;AACnD,OAAO,KAAKC,KAAK,MAAM,cAAc;AAErC,OAAOC,OAAO,MAAM,WAAW;AAC/B,OAAOC,WAAW,MAAM,eAAe;AACvC,OAAOC,UAAU,MAAM,cAAc;AACrC,OAAOC,gBAAgB,MAAM,sBAAsB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAMC,WAAW,GAAG;EAClBC,IAAI,EAAEL,OAAO;EACbM,QAAQ,EAAEL,WAAW;EACrBM,OAAO,EAAEL;AACX,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,gBAAiBM,cAAcA,CAAEC,IAAI,EAAEC,UAAU,EAAEC,OAAO,EAAE;EAC1D,IAAIC,KAAK,GAAG,CAAC,CAAC;EACd,IAAIC,QAAQ;EACZ,IAAIC,cAAc;EAElB,IAAI,OAAOH,OAAO,CAACG,cAAc,KAAK,UAAU,EAAE;IAChDA,cAAc,GAAGH,OAAO,CAACG,cAAc;EACzC,CAAC,MAAM;IACLA,cAAc,GAAGX,gBAAgB;EACnC;EAEA,WAAW,MAAMY,KAAK,IAAIlB,aAAa,CAACiB,cAAc,CAACL,IAAI,EAAEC,UAAU,EAAEC,OAAO,CAAC,EAAEA,OAAO,CAACK,qBAAqB,CAAC,EAAE;IACjHJ,KAAK,EAAE;IAEP,IAAIA,KAAK,KAAK,CAAC,EAAE;MACfC,QAAQ,GAAGE,KAAK;MAChB;IACF,CAAC,MAAM,IAAIH,KAAK,KAAK,CAAC,IAAIC,QAAQ,EAAE;MAClC,MAAMA,QAAQ;MACdA,QAAQ,GAAG,IAAI;IACjB;IAEA,MAAME,KAAK;EACb;EAEA,IAAIF,QAAQ,EAAE;IACZA,QAAQ,CAACI,MAAM,GAAG,IAAI;IACtB,MAAMJ,QAAQ;EAChB;AACF;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAMK,MAAM,GAAGA,CAACT,IAAI,EAAEC,UAAU,EAAEC,OAAO,KAAK;EAC5C;AACF;AACA;EACE,eAAeQ,OAAOA,CAAEC,MAAM,EAAE;IAC9B,IAAIA,MAAM,CAACC,MAAM,KAAK,CAAC,IAAID,MAAM,CAAC,CAAC,CAAC,CAACH,MAAM,IAAIN,OAAO,CAACW,sBAAsB,EAAE;MAC7E,MAAMC,IAAI,GAAGH,MAAM,CAAC,CAAC,CAAC;MAEtB,IAAIX,IAAI,CAACe,KAAK,KAAKC,SAAS,IAAIhB,IAAI,CAACiB,IAAI,KAAKD,SAAS,EAAE;QACvD;QACA;QACA,IAAIE,MAAM,GAAG,MAAMjB,UAAU,CAACkB,GAAG,CAACL,IAAI,CAACM,GAAG,CAAC;QAE3CN,IAAI,CAACO,MAAM,GAAG,IAAIrC,MAAM,CAAC;UACvBsC,IAAI,EAAE,MAAM;UACZP,KAAK,EAAEf,IAAI,CAACe,KAAK;UACjBE,IAAI,EAAEjB,IAAI,CAACiB,IAAI;UACfM,IAAI,EAAEL;QACR,CAAC,CAAC;QAEFA,MAAM,GAAGhC,MAAM,CAACC,OAAO,CAAC;UAAEqC,IAAI,EAAEV,IAAI,CAACO,MAAM,CAACI,OAAO,CAAC;QAAE,CAAC,CAAC,CAAC;;QAEzD;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACAX,IAAI,CAACM,GAAG,GAAG,MAAMnC,OAAO,CAACiC,MAAM,EAAEjB,UAAU,EAAE;UAC3C,GAAGC,OAAO;UACVwB,KAAK,EAAEpC,KAAK;UACZqC,MAAM,EAAEzB,OAAO,CAACyB,MAAM;UACtBC,UAAU,EAAE1B,OAAO,CAAC0B;QACtB,CAAC,CAAC;QACFd,IAAI,CAACe,IAAI,GAAGX,MAAM,CAACN,MAAM;MAC3B;MAEA,OAAO;QACLQ,GAAG,EAAEN,IAAI,CAACM,GAAG;QACbU,IAAI,EAAE9B,IAAI,CAAC8B,IAAI;QACfT,MAAM,EAAEP,IAAI,CAACO,MAAM;QACnBQ,IAAI,EAAEf,IAAI,CAACe;MACb,CAAC;IACH;;IAEA;IACA,MAAME,CAAC,GAAG,IAAI/C,MAAM,CAAC;MACnBsC,IAAI,EAAE,MAAM;MACZP,KAAK,EAAEf,IAAI,CAACe,KAAK;MACjBE,IAAI,EAAEjB,IAAI,CAACiB;IACb,CAAC,CAAC;IAEF,MAAMe,KAAK,GAAGrB,MAAM,CACjBsB,MAAM,CAACnB,IAAI,IAAI;MACd,IAAIA,IAAI,CAACM,GAAG,CAACc,IAAI,KAAK7C,QAAQ,CAAC6C,IAAI,IAAIpB,IAAI,CAACe,IAAI,EAAE;QAChD,OAAO,IAAI;MACb;MAEA,IAAIf,IAAI,CAACO,MAAM,IAAI,CAACP,IAAI,CAACO,MAAM,CAACE,IAAI,IAAIT,IAAI,CAACO,MAAM,CAACc,QAAQ,CAAC,CAAC,EAAE;QAC9D,OAAO,IAAI;MACb;MAEA,OAAOC,OAAO,CAACtB,IAAI,CAACO,MAAM,IAAIP,IAAI,CAACO,MAAM,CAACE,IAAI,IAAIT,IAAI,CAACO,MAAM,CAACE,IAAI,CAACX,MAAM,CAAC;IAC5E,CAAC,CAAC,CACDyB,GAAG,CAAEvB,IAAI,IAAK;MACb,IAAIA,IAAI,CAACM,GAAG,CAACc,IAAI,KAAK7C,QAAQ,CAAC6C,IAAI,EAAE;QACnC;QACAH,CAAC,CAACO,YAAY,CAACxB,IAAI,CAACe,IAAI,CAAC;QAEzB,OAAO;UACLU,IAAI,EAAE,EAAE;UACRC,KAAK,EAAE1B,IAAI,CAACe,IAAI;UAChBY,IAAI,EAAE3B,IAAI,CAACM;QACb,CAAC;MACH;MAEA,IAAI,CAACN,IAAI,CAACO,MAAM,IAAI,CAACP,IAAI,CAACO,MAAM,CAACE,IAAI,EAAE;QACrC;QACAQ,CAAC,CAACO,YAAY,CAAExB,IAAI,CAACO,MAAM,IAAIP,IAAI,CAACO,MAAM,CAACc,QAAQ,CAAC,CAAC,IAAK,CAAC,CAAC;MAC9D,CAAC,MAAM;QACL;QACAJ,CAAC,CAACO,YAAY,CAACxB,IAAI,CAACO,MAAM,CAACE,IAAI,CAACX,MAAM,CAAC;MACzC;MAEA,OAAO;QACL2B,IAAI,EAAE,EAAE;QACRC,KAAK,EAAE1B,IAAI,CAACe,IAAI;QAChBY,IAAI,EAAE3B,IAAI,CAACM;MACb,CAAC;IACH,CAAC,CAAC;IAEJ,MAAMsB,IAAI,GAAG;MACXlB,IAAI,EAAEO,CAAC,CAACN,OAAO,CAAC,CAAC;MACjBkB,KAAK,EAAEX;IACT,CAAC;IACD,MAAMd,MAAM,GAAGhC,MAAM,CAACC,OAAO,CAACuD,IAAI,CAAC,CAAC;IACpC,MAAMtB,GAAG,GAAG,MAAMnC,OAAO,CAACiC,MAAM,EAAEjB,UAAU,EAAEC,OAAO,CAAC;IAEtD,OAAO;MACLkB,GAAG;MACHU,IAAI,EAAE9B,IAAI,CAAC8B,IAAI;MACfT,MAAM,EAAEU,CAAC;MACTF,IAAI,EAAEX,MAAM,CAACN,MAAM,GAAG8B,IAAI,CAACC,KAAK,CAAClC,MAAM,CAAC,CAACmC,GAAG,EAAEC,IAAI,KAAKD,GAAG,GAAGC,IAAI,CAACL,KAAK,EAAE,CAAC;IAC5E,CAAC;EACH;EAEA,OAAO9B,OAAO;AAChB,CAAC;;AAED;AACA;AACA;AACA,SAASoC,WAAWA,CAAE9C,IAAI,EAAE+C,KAAK,EAAE7C,OAAO,EAAE;EAC1C,MAAM8C,UAAU,GAAGrD,WAAW,CAACO,OAAO,CAAC+C,QAAQ,CAAC;EAEhD,IAAI,CAACD,UAAU,EAAE;IACf,MAAMjE,OAAO,CAAC,IAAImE,KAAK,CAAE,yCAAwChD,OAAO,CAAC+C,QAAS,EAAC,CAAC,EAAE,kBAAkB,CAAC;EAC3G;EAEA,OAAOD,UAAU,CAACjD,cAAc,CAACC,IAAI,EAAE+C,KAAK,EAAE7C,OAAO,CAAC,EAAEO,MAAM,CAACT,IAAI,EAAE+C,KAAK,EAAE7C,OAAO,CAAC,EAAEA,OAAO,CAAC;AAChG;AAEA,eAAe4C,WAAW"},"metadata":{},"sourceType":"module","externalDependencies":[]}