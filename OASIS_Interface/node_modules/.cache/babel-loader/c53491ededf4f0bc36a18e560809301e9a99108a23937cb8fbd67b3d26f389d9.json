{"ast":null,"code":"import \"core-js/modules/es.typed-array.to-reversed.js\";\nimport \"core-js/modules/es.typed-array.to-sorted.js\";\nimport \"core-js/modules/es.typed-array.with.js\";\nimport \"core-js/modules/es.array.push.js\";\nimport * as dagPB from '@ipld/dag-pb';\nimport { Bucket, createHAMT } from 'hamt-sharding';\nimport { DirSharded } from './dir-sharded.js';\nimport { logger } from '@libp2p/logger';\nimport { UnixFS } from 'ipfs-unixfs';\nimport last from 'it-last';\nimport { CID } from 'multiformats/cid';\nimport { hamtHashCode, hamtHashFn, hamtBucketBits } from './hamt-constants.js';\nconst log = logger('ipfs:mfs:core:utils:hamt-utils');\n\n/**\n * @typedef {import('multiformats/cid').Version} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\nexport const updateHamtDirectory = async (context, links, bucket, options) => {\n  if (!options.parent.Data) {\n    throw new Error('Could not update HAMT directory because parent had no data');\n  }\n\n  // update parent with new bit field\n  const data = Uint8Array.from(bucket._children.bitField().reverse());\n  const node = UnixFS.unmarshal(options.parent.Data);\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: hamtHashCode,\n    mode: node.mode,\n    mtime: node.mtime\n  });\n  const hasher = await context.hashers.getHasher(options.hashAlg);\n  const parent = {\n    Data: dir.marshal(),\n    Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n  };\n  const buf = dagPB.encode(parent);\n  const hash = await hasher.digest(buf);\n  const cid = CID.create(options.cidVersion, dagPB.code, hash);\n  if (options.flush) {\n    await context.repo.blocks.put(cid, buf);\n  }\n  return {\n    node: parent,\n    cid,\n    size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n  };\n};\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\nexport const recreateHamtLevel = async (context, links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hash: rootBucket._options.hash,\n    bits: rootBucket._options.bits\n  }, parentBucket, positionAtParent);\n  parentBucket._putObjectAt(positionAtParent, bucket);\n  await addLinksToHamtBucket(context, links, bucket, rootBucket);\n  return bucket;\n};\n\n/**\n * @param {PBLink[]} links\n */\nexport const recreateInitialHamtLevel = async links => {\n  const bucket = createHAMT({\n    hashFn: hamtHashFn,\n    bits: hamtBucketBits\n  });\n\n  // populate sub bucket but do not recurse as we do not want to pull whole shard in\n  await Promise.all(links.map(async link => {\n    const linkName = link.Name || '';\n    if (linkName.length === 2) {\n      const pos = parseInt(linkName, 16);\n      const subBucket = new Bucket({\n        hash: bucket._options.hash,\n        bits: bucket._options.bits\n      }, bucket, pos);\n      bucket._putObjectAt(pos, subBucket);\n      return Promise.resolve();\n    }\n    return bucket.put(linkName.substring(2), {\n      size: link.Tsize,\n      cid: link.Hash\n    });\n  }));\n  return bucket;\n};\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\nexport const addLinksToHamtBucket = async (context, links, bucket, rootBucket) => {\n  await Promise.all(links.map(async link => {\n    const linkName = link.Name || '';\n    if (linkName.length === 2) {\n      log('Populating sub bucket', linkName);\n      const pos = parseInt(linkName, 16);\n      const block = await context.repo.blocks.get(link.Hash);\n      const node = dagPB.decode(block);\n      const subBucket = new Bucket({\n        hash: rootBucket._options.hash,\n        bits: rootBucket._options.bits\n      }, bucket, pos);\n      bucket._putObjectAt(pos, subBucket);\n      await addLinksToHamtBucket(context, node.Links, subBucket, rootBucket);\n      return Promise.resolve();\n    }\n    return rootBucket.put(linkName.substring(2), {\n      size: link.Tsize,\n      cid: link.Hash\n    });\n  }));\n};\n\n/**\n * @param {number} position\n */\nexport const toPrefix = position => {\n  return position.toString(16).toUpperCase().padStart(2, '0').substring(0, 2);\n};\n\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\nexport const generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateInitialHamtLevel(rootNode.Links);\n  const position = await rootBucket._findNewBucketAndPos(fileName);\n\n  // the path to the root bucket\n  /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }];\n  let currentBucket = position.bucket;\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    });\n\n    // @ts-expect-error - only the root bucket's parent will be undefined\n    currentBucket = currentBucket._parent;\n  }\n  path.reverse();\n  path[0].node = rootNode;\n\n  // load PbNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i];\n    if (!segment.node) {\n      throw new Error('Could not generate HAMT path');\n    }\n\n    // find prefix in links\n    const link = segment.node.Links.filter(link => (link.Name || '').substring(0, 2) === segment.prefix).pop();\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`);\n      // return path\n      continue;\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`);\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue;\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`);\n    const block = await context.repo.blocks.get(link.Hash);\n    const node = dagPB.decode(block);\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(context, node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(fileName);\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      });\n      continue;\n    }\n    const nextSegment = path[i + 1];\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(context, node.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = node;\n  }\n  await rootBucket.put(fileName, true);\n  path.reverse();\n  return {\n    rootBucket,\n    path\n  };\n};\n\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\nexport const createShard = async (context, contents, options = {}) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: undefined,\n    parentKey: undefined,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, options);\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    });\n  }\n  const res = await last(shard.flush(context.repo.blocks));\n  if (!res) {\n    throw new Error('Flushing shard yielded no result');\n  }\n  return res;\n};","map":{"version":3,"names":["dagPB","Bucket","createHAMT","DirSharded","logger","UnixFS","last","CID","hamtHashCode","hamtHashFn","hamtBucketBits","log","updateHamtDirectory","context","links","bucket","options","parent","Data","Error","data","Uint8Array","from","_children","bitField","reverse","node","unmarshal","dir","type","fanout","tableSize","hashType","mode","mtime","hasher","hashers","getHasher","hashAlg","marshal","Links","sort","a","b","Name","localeCompare","buf","encode","hash","digest","cid","create","cidVersion","code","flush","repo","blocks","put","size","reduce","sum","link","Tsize","length","recreateHamtLevel","rootBucket","parentBucket","positionAtParent","_options","bits","_putObjectAt","addLinksToHamtBucket","recreateInitialHamtLevel","hashFn","Promise","all","map","linkName","pos","parseInt","subBucket","resolve","substring","Hash","block","get","decode","toPrefix","position","toString","toUpperCase","padStart","generatePath","fileName","rootNode","_findNewBucketAndPos","path","prefix","currentBucket","push","_posAtParent","_parent","i","segment","filter","pop","nextSegment","createShard","contents","shard","root","undefined","parentKey","dirty","flat","_bucket","name","res"],"sources":["/Users/yezery/Oasis/OASIS/node_modules/.store/ipfs-core@0.18.1/node_modules/ipfs-core/src/components/files/utils/hamt-utils.js"],"sourcesContent":["import * as dagPB from '@ipld/dag-pb'\nimport {\n  Bucket,\n  createHAMT\n} from 'hamt-sharding'\nimport { DirSharded } from './dir-sharded.js'\nimport { logger } from '@libp2p/logger'\nimport { UnixFS } from 'ipfs-unixfs'\nimport last from 'it-last'\nimport { CID } from 'multiformats/cid'\nimport {\n  hamtHashCode,\n  hamtHashFn,\n  hamtBucketBits\n} from './hamt-constants.js'\n\nconst log = logger('ipfs:mfs:core:utils:hamt-utils')\n\n/**\n * @typedef {import('multiformats/cid').Version} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\nexport const updateHamtDirectory = async (context, links, bucket, options) => {\n  if (!options.parent.Data) {\n    throw new Error('Could not update HAMT directory because parent had no data')\n  }\n\n  // update parent with new bit field\n  const data = Uint8Array.from(bucket._children.bitField().reverse())\n  const node = UnixFS.unmarshal(options.parent.Data)\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: hamtHashCode,\n    mode: node.mode,\n    mtime: node.mtime\n  })\n\n  const hasher = await context.hashers.getHasher(options.hashAlg)\n  const parent = {\n    Data: dir.marshal(),\n    Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n  }\n  const buf = dagPB.encode(parent)\n  const hash = await hasher.digest(buf)\n  const cid = CID.create(options.cidVersion, dagPB.code, hash)\n\n  if (options.flush) {\n    await context.repo.blocks.put(cid, buf)\n  }\n\n  return {\n    node: parent,\n    cid,\n    size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n  }\n}\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\nexport const recreateHamtLevel = async (context, links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hash: rootBucket._options.hash,\n    bits: rootBucket._options.bits\n  }, parentBucket, positionAtParent)\n  parentBucket._putObjectAt(positionAtParent, bucket)\n\n  await addLinksToHamtBucket(context, links, bucket, rootBucket)\n\n  return bucket\n}\n\n/**\n * @param {PBLink[]} links\n */\nexport const recreateInitialHamtLevel = async (links) => {\n  const bucket = createHAMT({\n    hashFn: hamtHashFn,\n    bits: hamtBucketBits\n  })\n\n  // populate sub bucket but do not recurse as we do not want to pull whole shard in\n  await Promise.all(\n    links.map(async link => {\n      const linkName = (link.Name || '')\n\n      if (linkName.length === 2) {\n        const pos = parseInt(linkName, 16)\n\n        const subBucket = new Bucket({\n          hash: bucket._options.hash,\n          bits: bucket._options.bits\n        }, bucket, pos)\n        bucket._putObjectAt(pos, subBucket)\n\n        return Promise.resolve()\n      }\n\n      return bucket.put(linkName.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      })\n    })\n  )\n\n  return bucket\n}\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\nexport const addLinksToHamtBucket = async (context, links, bucket, rootBucket) => {\n  await Promise.all(\n    links.map(async link => {\n      const linkName = (link.Name || '')\n\n      if (linkName.length === 2) {\n        log('Populating sub bucket', linkName)\n        const pos = parseInt(linkName, 16)\n        const block = await context.repo.blocks.get(link.Hash)\n        const node = dagPB.decode(block)\n\n        const subBucket = new Bucket({\n          hash: rootBucket._options.hash,\n          bits: rootBucket._options.bits\n        }, bucket, pos)\n        bucket._putObjectAt(pos, subBucket)\n\n        await addLinksToHamtBucket(context, node.Links, subBucket, rootBucket)\n\n        return Promise.resolve()\n      }\n\n      return rootBucket.put(linkName.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      })\n    })\n  )\n}\n\n/**\n * @param {number} position\n */\nexport const toPrefix = (position) => {\n  return position\n    .toString(16)\n    .toUpperCase()\n    .padStart(2, '0')\n    .substring(0, 2)\n}\n\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\nexport const generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateInitialHamtLevel(rootNode.Links)\n  const position = await rootBucket._findNewBucketAndPos(fileName)\n\n  // the path to the root bucket\n  /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n  let currentBucket = position.bucket\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    })\n\n    // @ts-expect-error - only the root bucket's parent will be undefined\n    currentBucket = currentBucket._parent\n  }\n\n  path.reverse()\n  path[0].node = rootNode\n\n  // load PbNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]\n\n    if (!segment.node) {\n      throw new Error('Could not generate HAMT path')\n    }\n\n    // find prefix in links\n    const link = segment.node.Links\n      .filter(link => (link.Name || '').substring(0, 2) === segment.prefix)\n      .pop()\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`)\n      // return path\n      continue\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`)\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`)\n    const block = await context.repo.blocks.get(link.Hash)\n    const node = dagPB.decode(block)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n\n      await recreateHamtLevel(context, node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n      const position = await rootBucket._findNewBucketAndPos(fileName)\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      })\n\n      continue\n    }\n\n    const nextSegment = path[i + 1]\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(context, node.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = node\n  }\n\n  await rootBucket.put(fileName, true)\n\n  path.reverse()\n\n  return {\n    rootBucket,\n    path\n  }\n}\n\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\nexport const createShard = async (context, contents, options = {}) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: undefined,\n    parentKey: undefined,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, options)\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    })\n  }\n\n  const res = await last(shard.flush(context.repo.blocks))\n\n  if (!res) {\n    throw new Error('Flushing shard yielded no result')\n  }\n\n  return res\n}\n"],"mappings":";;;;AAAA,OAAO,KAAKA,KAAK,MAAM,cAAc;AACrC,SACEC,MAAM,EACNC,UAAU,QACL,eAAe;AACtB,SAASC,UAAU,QAAQ,kBAAkB;AAC7C,SAASC,MAAM,QAAQ,gBAAgB;AACvC,SAASC,MAAM,QAAQ,aAAa;AACpC,OAAOC,IAAI,MAAM,SAAS;AAC1B,SAASC,GAAG,QAAQ,kBAAkB;AACtC,SACEC,YAAY,EACZC,UAAU,EACVC,cAAc,QACT,qBAAqB;AAE5B,MAAMC,GAAG,GAAGP,MAAM,CAAC,gCAAgC,CAAC;;AAEpD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMQ,mBAAmB,GAAG,MAAAA,CAAOC,OAAO,EAAEC,KAAK,EAAEC,MAAM,EAAEC,OAAO,KAAK;EAC5E,IAAI,CAACA,OAAO,CAACC,MAAM,CAACC,IAAI,EAAE;IACxB,MAAM,IAAIC,KAAK,CAAC,4DAA4D,CAAC;EAC/E;;EAEA;EACA,MAAMC,IAAI,GAAGC,UAAU,CAACC,IAAI,CAACP,MAAM,CAACQ,SAAS,CAACC,QAAQ,CAAC,CAAC,CAACC,OAAO,CAAC,CAAC,CAAC;EACnE,MAAMC,IAAI,GAAGrB,MAAM,CAACsB,SAAS,CAACX,OAAO,CAACC,MAAM,CAACC,IAAI,CAAC;EAClD,MAAMU,GAAG,GAAG,IAAIvB,MAAM,CAAC;IACrBwB,IAAI,EAAE,wBAAwB;IAC9BT,IAAI;IACJU,MAAM,EAAEf,MAAM,CAACgB,SAAS,CAAC,CAAC;IAC1BC,QAAQ,EAAExB,YAAY;IACtByB,IAAI,EAAEP,IAAI,CAACO,IAAI;IACfC,KAAK,EAAER,IAAI,CAACQ;EACd,CAAC,CAAC;EAEF,MAAMC,MAAM,GAAG,MAAMtB,OAAO,CAACuB,OAAO,CAACC,SAAS,CAACrB,OAAO,CAACsB,OAAO,CAAC;EAC/D,MAAMrB,MAAM,GAAG;IACbC,IAAI,EAAEU,GAAG,CAACW,OAAO,CAAC,CAAC;IACnBC,KAAK,EAAE1B,KAAK,CAAC2B,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAK,CAACD,CAAC,CAACE,IAAI,IAAI,EAAE,EAAEC,aAAa,CAACF,CAAC,CAACC,IAAI,IAAI,EAAE,CAAC;EACxE,CAAC;EACD,MAAME,GAAG,GAAG9C,KAAK,CAAC+C,MAAM,CAAC9B,MAAM,CAAC;EAChC,MAAM+B,IAAI,GAAG,MAAMb,MAAM,CAACc,MAAM,CAACH,GAAG,CAAC;EACrC,MAAMI,GAAG,GAAG3C,GAAG,CAAC4C,MAAM,CAACnC,OAAO,CAACoC,UAAU,EAAEpD,KAAK,CAACqD,IAAI,EAAEL,IAAI,CAAC;EAE5D,IAAIhC,OAAO,CAACsC,KAAK,EAAE;IACjB,MAAMzC,OAAO,CAAC0C,IAAI,CAACC,MAAM,CAACC,GAAG,CAACP,GAAG,EAAEJ,GAAG,CAAC;EACzC;EAEA,OAAO;IACLpB,IAAI,EAAET,MAAM;IACZiC,GAAG;IACHQ,IAAI,EAAE5C,KAAK,CAAC6C,MAAM,CAAC,CAACC,GAAG,EAAEC,IAAI,KAAKD,GAAG,IAAIC,IAAI,CAACC,KAAK,IAAI,CAAC,CAAC,EAAEhB,GAAG,CAACiB,MAAM;EACvE,CAAC;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,iBAAiB,GAAG,MAAAA,CAAOnD,OAAO,EAAEC,KAAK,EAAEmD,UAAU,EAAEC,YAAY,EAAEC,gBAAgB,KAAK;EACrG;EACA,MAAMpD,MAAM,GAAG,IAAId,MAAM,CAAC;IACxB+C,IAAI,EAAEiB,UAAU,CAACG,QAAQ,CAACpB,IAAI;IAC9BqB,IAAI,EAAEJ,UAAU,CAACG,QAAQ,CAACC;EAC5B,CAAC,EAAEH,YAAY,EAAEC,gBAAgB,CAAC;EAClCD,YAAY,CAACI,YAAY,CAACH,gBAAgB,EAAEpD,MAAM,CAAC;EAEnD,MAAMwD,oBAAoB,CAAC1D,OAAO,EAAEC,KAAK,EAAEC,MAAM,EAAEkD,UAAU,CAAC;EAE9D,OAAOlD,MAAM;AACf,CAAC;;AAED;AACA;AACA;AACA,OAAO,MAAMyD,wBAAwB,GAAG,MAAO1D,KAAK,IAAK;EACvD,MAAMC,MAAM,GAAGb,UAAU,CAAC;IACxBuE,MAAM,EAAEhE,UAAU;IAClB4D,IAAI,EAAE3D;EACR,CAAC,CAAC;;EAEF;EACA,MAAMgE,OAAO,CAACC,GAAG,CACf7D,KAAK,CAAC8D,GAAG,CAAC,MAAMf,IAAI,IAAI;IACtB,MAAMgB,QAAQ,GAAIhB,IAAI,CAACjB,IAAI,IAAI,EAAG;IAElC,IAAIiC,QAAQ,CAACd,MAAM,KAAK,CAAC,EAAE;MACzB,MAAMe,GAAG,GAAGC,QAAQ,CAACF,QAAQ,EAAE,EAAE,CAAC;MAElC,MAAMG,SAAS,GAAG,IAAI/E,MAAM,CAAC;QAC3B+C,IAAI,EAAEjC,MAAM,CAACqD,QAAQ,CAACpB,IAAI;QAC1BqB,IAAI,EAAEtD,MAAM,CAACqD,QAAQ,CAACC;MACxB,CAAC,EAAEtD,MAAM,EAAE+D,GAAG,CAAC;MACf/D,MAAM,CAACuD,YAAY,CAACQ,GAAG,EAAEE,SAAS,CAAC;MAEnC,OAAON,OAAO,CAACO,OAAO,CAAC,CAAC;IAC1B;IAEA,OAAOlE,MAAM,CAAC0C,GAAG,CAACoB,QAAQ,CAACK,SAAS,CAAC,CAAC,CAAC,EAAE;MACvCxB,IAAI,EAAEG,IAAI,CAACC,KAAK;MAChBZ,GAAG,EAAEW,IAAI,CAACsB;IACZ,CAAC,CAAC;EACJ,CAAC,CACH,CAAC;EAED,OAAOpE,MAAM;AACf,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMwD,oBAAoB,GAAG,MAAAA,CAAO1D,OAAO,EAAEC,KAAK,EAAEC,MAAM,EAAEkD,UAAU,KAAK;EAChF,MAAMS,OAAO,CAACC,GAAG,CACf7D,KAAK,CAAC8D,GAAG,CAAC,MAAMf,IAAI,IAAI;IACtB,MAAMgB,QAAQ,GAAIhB,IAAI,CAACjB,IAAI,IAAI,EAAG;IAElC,IAAIiC,QAAQ,CAACd,MAAM,KAAK,CAAC,EAAE;MACzBpD,GAAG,CAAC,uBAAuB,EAAEkE,QAAQ,CAAC;MACtC,MAAMC,GAAG,GAAGC,QAAQ,CAACF,QAAQ,EAAE,EAAE,CAAC;MAClC,MAAMO,KAAK,GAAG,MAAMvE,OAAO,CAAC0C,IAAI,CAACC,MAAM,CAAC6B,GAAG,CAACxB,IAAI,CAACsB,IAAI,CAAC;MACtD,MAAMzD,IAAI,GAAG1B,KAAK,CAACsF,MAAM,CAACF,KAAK,CAAC;MAEhC,MAAMJ,SAAS,GAAG,IAAI/E,MAAM,CAAC;QAC3B+C,IAAI,EAAEiB,UAAU,CAACG,QAAQ,CAACpB,IAAI;QAC9BqB,IAAI,EAAEJ,UAAU,CAACG,QAAQ,CAACC;MAC5B,CAAC,EAAEtD,MAAM,EAAE+D,GAAG,CAAC;MACf/D,MAAM,CAACuD,YAAY,CAACQ,GAAG,EAAEE,SAAS,CAAC;MAEnC,MAAMT,oBAAoB,CAAC1D,OAAO,EAAEa,IAAI,CAACc,KAAK,EAAEwC,SAAS,EAAEf,UAAU,CAAC;MAEtE,OAAOS,OAAO,CAACO,OAAO,CAAC,CAAC;IAC1B;IAEA,OAAOhB,UAAU,CAACR,GAAG,CAACoB,QAAQ,CAACK,SAAS,CAAC,CAAC,CAAC,EAAE;MAC3CxB,IAAI,EAAEG,IAAI,CAACC,KAAK;MAChBZ,GAAG,EAAEW,IAAI,CAACsB;IACZ,CAAC,CAAC;EACJ,CAAC,CACH,CAAC;AACH,CAAC;;AAED;AACA;AACA;AACA,OAAO,MAAMI,QAAQ,GAAIC,QAAQ,IAAK;EACpC,OAAOA,QAAQ,CACZC,QAAQ,CAAC,EAAE,CAAC,CACZC,WAAW,CAAC,CAAC,CACbC,QAAQ,CAAC,CAAC,EAAE,GAAG,CAAC,CAChBT,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC;AACpB,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMU,YAAY,GAAG,MAAAA,CAAO/E,OAAO,EAAEgF,QAAQ,EAAEC,QAAQ,KAAK;EACjE;EACA,MAAM7B,UAAU,GAAG,MAAMO,wBAAwB,CAACsB,QAAQ,CAACtD,KAAK,CAAC;EACjE,MAAMgD,QAAQ,GAAG,MAAMvB,UAAU,CAAC8B,oBAAoB,CAACF,QAAQ,CAAC;;EAEhE;EACA;EACA,MAAMG,IAAI,GAAG,CAAC;IACZjF,MAAM,EAAEyE,QAAQ,CAACzE,MAAM;IACvBkF,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACV,GAAG;EAC/B,CAAC,CAAC;EACF,IAAIoB,aAAa,GAAGV,QAAQ,CAACzE,MAAM;EAEnC,OAAOmF,aAAa,KAAKjC,UAAU,EAAE;IACnC+B,IAAI,CAACG,IAAI,CAAC;MACRpF,MAAM,EAAEmF,aAAa;MACrBD,MAAM,EAAEV,QAAQ,CAACW,aAAa,CAACE,YAAY;IAC7C,CAAC,CAAC;;IAEF;IACAF,aAAa,GAAGA,aAAa,CAACG,OAAO;EACvC;EAEAL,IAAI,CAACvE,OAAO,CAAC,CAAC;EACduE,IAAI,CAAC,CAAC,CAAC,CAACtE,IAAI,GAAGoE,QAAQ;;EAEvB;EACA,KAAK,IAAIQ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,IAAI,CAACjC,MAAM,EAAEuC,CAAC,EAAE,EAAE;IACpC,MAAMC,OAAO,GAAGP,IAAI,CAACM,CAAC,CAAC;IAEvB,IAAI,CAACC,OAAO,CAAC7E,IAAI,EAAE;MACjB,MAAM,IAAIP,KAAK,CAAC,8BAA8B,CAAC;IACjD;;IAEA;IACA,MAAM0C,IAAI,GAAG0C,OAAO,CAAC7E,IAAI,CAACc,KAAK,CAC5BgE,MAAM,CAAC3C,IAAI,IAAI,CAACA,IAAI,CAACjB,IAAI,IAAI,EAAE,EAAEsC,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,KAAKqB,OAAO,CAACN,MAAM,CAAC,CACpEQ,GAAG,CAAC,CAAC;;IAER;IACA,IAAI,CAAC5C,IAAI,EAAE;MACT;MACAlD,GAAG,CAAE,QAAO4F,OAAO,CAACN,MAAO,GAAEJ,QAAS,gBAAe,CAAC;MACtD;MACA;IACF;;IAEA;IACA,IAAIhC,IAAI,CAACjB,IAAI,KAAM,GAAE2D,OAAO,CAACN,MAAO,GAAEJ,QAAS,EAAC,EAAE;MAChDlF,GAAG,CAAE,QAAO4F,OAAO,CAACN,MAAO,GAAEJ,QAAS,mBAAkB,CAAC;MACzD;MACA;MACA;IACF;;IAEA;IACAlF,GAAG,CAAE,kBAAiB4F,OAAO,CAACN,MAAO,EAAC,CAAC;IACvC,MAAMb,KAAK,GAAG,MAAMvE,OAAO,CAAC0C,IAAI,CAACC,MAAM,CAAC6B,GAAG,CAACxB,IAAI,CAACsB,IAAI,CAAC;IACtD,MAAMzD,IAAI,GAAG1B,KAAK,CAACsF,MAAM,CAACF,KAAK,CAAC;;IAEhC;IACA,IAAI,CAACY,IAAI,CAACM,CAAC,GAAG,CAAC,CAAC,EAAE;MAChB3F,GAAG,CAAE,uBAAsB4F,OAAO,CAACN,MAAO,EAAC,CAAC;MAE5C,MAAMjC,iBAAiB,CAACnD,OAAO,EAAEa,IAAI,CAACc,KAAK,EAAEyB,UAAU,EAAEsC,OAAO,CAACxF,MAAM,EAAEgE,QAAQ,CAACwB,OAAO,CAACN,MAAM,EAAE,EAAE,CAAC,CAAC;MACtG,MAAMT,QAAQ,GAAG,MAAMvB,UAAU,CAAC8B,oBAAoB,CAACF,QAAQ,CAAC;;MAEhE;MACAG,IAAI,CAACG,IAAI,CAAC;QACRpF,MAAM,EAAEyE,QAAQ,CAACzE,MAAM;QACvBkF,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACV,GAAG,CAAC;QAC9BpD,IAAI,EAAEA;MACR,CAAC,CAAC;MAEF;IACF;IAEA,MAAMgF,WAAW,GAAGV,IAAI,CAACM,CAAC,GAAG,CAAC,CAAC;;IAE/B;IACA,MAAM/B,oBAAoB,CAAC1D,OAAO,EAAEa,IAAI,CAACc,KAAK,EAAEkE,WAAW,CAAC3F,MAAM,EAAEkD,UAAU,CAAC;IAE/EyC,WAAW,CAAChF,IAAI,GAAGA,IAAI;EACzB;EAEA,MAAMuC,UAAU,CAACR,GAAG,CAACoC,QAAQ,EAAE,IAAI,CAAC;EAEpCG,IAAI,CAACvE,OAAO,CAAC,CAAC;EAEd,OAAO;IACLwC,UAAU;IACV+B;EACF,CAAC;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMW,WAAW,GAAG,MAAAA,CAAO9F,OAAO,EAAE+F,QAAQ,EAAE5F,OAAO,GAAG,CAAC,CAAC,KAAK;EACpE,MAAM6F,KAAK,GAAG,IAAI1G,UAAU,CAAC;IAC3B2G,IAAI,EAAE,IAAI;IACVlF,GAAG,EAAE,IAAI;IACTX,MAAM,EAAE8F,SAAS;IACjBC,SAAS,EAAED,SAAS;IACpBf,IAAI,EAAE,EAAE;IACRiB,KAAK,EAAE,IAAI;IACXC,IAAI,EAAE,KAAK;IACXhF,KAAK,EAAElB,OAAO,CAACkB,KAAK;IACpBD,IAAI,EAAEjB,OAAO,CAACiB;EAChB,CAAC,EAAEjB,OAAO,CAAC;EAEX,KAAK,IAAIsF,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGM,QAAQ,CAAC7C,MAAM,EAAEuC,CAAC,EAAE,EAAE;IACxC,MAAMO,KAAK,CAACM,OAAO,CAAC1D,GAAG,CAACmD,QAAQ,CAACN,CAAC,CAAC,CAACc,IAAI,EAAE;MACxC1D,IAAI,EAAEkD,QAAQ,CAACN,CAAC,CAAC,CAAC5C,IAAI;MACtBR,GAAG,EAAE0D,QAAQ,CAACN,CAAC,CAAC,CAACpD;IACnB,CAAC,CAAC;EACJ;EAEA,MAAMmE,GAAG,GAAG,MAAM/G,IAAI,CAACuG,KAAK,CAACvD,KAAK,CAACzC,OAAO,CAAC0C,IAAI,CAACC,MAAM,CAAC,CAAC;EAExD,IAAI,CAAC6D,GAAG,EAAE;IACR,MAAM,IAAIlG,KAAK,CAAC,kCAAkC,CAAC;EACrD;EAEA,OAAOkG,GAAG;AACZ,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}